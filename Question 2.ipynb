{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from operator import add\n",
    "\n",
    "conf = SparkConf().setAppName(\"Movies\")\n",
    "sc = SparkContext(conf = conf) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = sc.textFile('./moviedata/users.csv')\n",
    "df_zipcodes = sc.textFile('./moviedata/zipcodes.csv')\n",
    "df_rating = sc.textFile('./moviedata/rating.csv')\n",
    "df_movies = sc.textFile('./moviedata/movies.csv')\n",
    "\n",
    "# df_movie = sqlContext.read.format('com.databricks.spark.csv').load('./moviedata/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('94560', '780'),\n",
       " ('48825', '781'),\n",
       " ('77081', '783'),\n",
       " ('91040', '784'),\n",
       " ('23322', '785')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcode_user = df_users.map(lambda x: x.split(\",\")).map(lambda x:  (str(x[4])[1:-1],str(x[0])))\n",
    "zipcode_user.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2574', 'MA'),\n",
       " ('1886', 'MA'),\n",
       " ('1472', 'MA'),\n",
       " ('2671', 'MA'),\n",
       " ('2672', 'MA')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcode_state = df_zipcodes.map(lambda x: x.split(\",\")).map(lambda x:  (str(x[0])[1:-1], str(x[3])[1:-1]))\n",
    "zipcode_state.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10707', ('95', 'NY')),\n",
       " ('98101', ('6', 'WA')),\n",
       " ('11753', ('924', 'NY')),\n",
       " ('6811', ('403', 'CT')),\n",
       " ('37076', ('874', 'TN'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcode_user_state = zipcode_user.join(zipcode_state)\n",
    "zipcode_user_state.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('95', 'NY'), ('6', 'WA'), ('924', 'NY'), ('403', 'CT'), ('874', 'TN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_state = zipcode_user_state.map(lambda x: x[1])\n",
    "user_state.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('253', ('97', 4)),\n",
       " ('284', ('269', 4)),\n",
       " ('106', ('526', 4)),\n",
       " ('121', ('180', 3)),\n",
       " ('62', ('86', 2))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movieid_rating = df_rating.map(lambda x: x.split(\",\")).map(lambda x: (str(x[0]), (str(x[1]),int(x[2]))))\n",
    "user_movieid_rating.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('214', (('117', 4), 'NY')),\n",
       " ('214', (('137', 4), 'NY')),\n",
       " ('214', (('213', 4), 'NY')),\n",
       " ('214', (('302', 4), 'NY')),\n",
       " ('214', (('693', 3), 'NY'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movieid_rating_state = user_movieid_rating.join(user_state)\n",
    "user_movieid_rating_state.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('712', 'MD'), (3, 1)),\n",
       " (('375', 'MA'), (2, 1)),\n",
       " (('1090', 'TX'), (7, 3)),\n",
       " (('316', 'AZ'), (5, 1)),\n",
       " (('471', 'NY'), (44, 12))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieid_state_rating_count = user_movieid_rating_state.map(lambda x: ((x[1][0][0],x[1][1]),(x[1][0][1],1))).reduceByKey(lambda x, y: (x[0]+ y[0], x[1] + y[1]))\n",
    "movieid_state_rating_count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('712', ('MD', 3.0)),\n",
       " ('375', ('MA', 2.0)),\n",
       " ('1090', ('TX', 2.3333333333333335)),\n",
       " ('316', ('AZ', 5.0)),\n",
       " ('471', ('NY', 3.6666666666666665))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieid_state_avgrating = movieid_state_rating_count.map(lambda x: (x[0][0], (x[0][1], float(x[1][0])/x[1][1])))\n",
    "movieid_state_avgrating.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kar_le_dukhi(x):\n",
    "    y = x.split('\\\"')\n",
    "    return y[0].split(',')[0:1] + y[2].split(',')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_movies_cleaned = df_movies.map(lambda x : kar_le_dukhi(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('281', 2),\n",
       " ('295', 2),\n",
       " ('298', 2),\n",
       " ('300', 2),\n",
       " ('313', 2),\n",
       " ('314', 2),\n",
       " ('326', 2),\n",
       " ('328', 2),\n",
       " ('334', 2),\n",
       " ('339', 2),\n",
       " ('343', 2),\n",
       " ('349', 2),\n",
       " ('350', 2),\n",
       " ('353', 2),\n",
       " ('358', 2),\n",
       " ('362', 2),\n",
       " ('363', 2),\n",
       " ('373', 2),\n",
       " ('374', 2),\n",
       " ('380', 2),\n",
       " ('385', 2),\n",
       " ('388', 2),\n",
       " ('391', 2),\n",
       " ('397', 2),\n",
       " ('398', 2),\n",
       " ('399', 2),\n",
       " ('403', 2),\n",
       " ('405', 2),\n",
       " ('426', 2),\n",
       " ('431', 2),\n",
       " ('435', 2),\n",
       " ('449', 2),\n",
       " ('450', 2),\n",
       " ('452', 2),\n",
       " ('453', 2),\n",
       " ('456', 2),\n",
       " ('2', 2),\n",
       " ('4', 2),\n",
       " ('17', 2),\n",
       " ('21', 2),\n",
       " ('22', 2),\n",
       " ('24', 2),\n",
       " ('27', 2),\n",
       " ('28', 2),\n",
       " ('29', 2),\n",
       " ('33', 2),\n",
       " ('39', 2),\n",
       " ('50', 2),\n",
       " ('53', 2),\n",
       " ('54', 2),\n",
       " ('62', 2),\n",
       " ('68', 2),\n",
       " ('73', 2),\n",
       " ('74', 2),\n",
       " ('79', 2),\n",
       " ('80', 2),\n",
       " ('82', 2),\n",
       " ('92', 2),\n",
       " ('96', 2),\n",
       " ('101', 2),\n",
       " ('110', 2),\n",
       " ('117', 2),\n",
       " ('118', 2),\n",
       " ('121', 2),\n",
       " ('127', 2),\n",
       " ('128', 2),\n",
       " ('144', 2),\n",
       " ('145', 2),\n",
       " ('147', 2),\n",
       " ('148', 2),\n",
       " ('161', 2),\n",
       " ('164', 2),\n",
       " ('172', 2),\n",
       " ('173', 2),\n",
       " ('174', 2),\n",
       " ('176', 2),\n",
       " ('177', 2),\n",
       " ('181', 2),\n",
       " ('183', 2),\n",
       " ('184', 2),\n",
       " ('186', 2),\n",
       " ('187', 2),\n",
       " ('188', 2),\n",
       " ('195', 2),\n",
       " ('201', 2),\n",
       " ('207', 2),\n",
       " ('210', 2),\n",
       " ('222', 2),\n",
       " ('226', 2),\n",
       " ('227', 2),\n",
       " ('228', 2),\n",
       " ('229', 2),\n",
       " ('230', 2),\n",
       " ('231', 2),\n",
       " ('232', 2),\n",
       " ('233', 2),\n",
       " ('234', 2),\n",
       " ('235', 2),\n",
       " ('241', 2),\n",
       " ('247', 2),\n",
       " ('250', 2),\n",
       " ('252', 2),\n",
       " ('254', 2),\n",
       " ('257', 2),\n",
       " ('260', 2),\n",
       " ('263', 2),\n",
       " ('265', 2),\n",
       " ('266', 2),\n",
       " ('271', 2),\n",
       " ('273', 2),\n",
       " ('665', 2),\n",
       " ('668', 2),\n",
       " ('679', 2),\n",
       " ('680', 2),\n",
       " ('684', 2),\n",
       " ('685', 2),\n",
       " ('686', 2),\n",
       " ('689', 2),\n",
       " ('720', 2),\n",
       " ('743', 2),\n",
       " ('748', 2),\n",
       " ('751', 2),\n",
       " ('752', 2),\n",
       " ('1138', 2),\n",
       " ('1139', 2),\n",
       " ('1161', 2),\n",
       " ('1180', 2),\n",
       " ('1181', 2),\n",
       " ('1183', 2),\n",
       " ('1188', 2),\n",
       " ('1215', 2),\n",
       " ('1222', 2),\n",
       " ('755', 2),\n",
       " ('759', 2),\n",
       " ('761', 2),\n",
       " ('769', 2),\n",
       " ('771', 2),\n",
       " ('779', 2),\n",
       " ('797', 2),\n",
       " ('798', 2),\n",
       " ('802', 2),\n",
       " ('803', 2),\n",
       " ('806', 2),\n",
       " ('808', 2),\n",
       " ('809', 2),\n",
       " ('810', 2),\n",
       " ('825', 2),\n",
       " ('827', 2),\n",
       " ('829', 2),\n",
       " ('830', 2),\n",
       " ('831', 2),\n",
       " ('833', 2),\n",
       " ('838', 2),\n",
       " ('840', 2),\n",
       " ('841', 2),\n",
       " ('849', 2),\n",
       " ('855', 2),\n",
       " ('876', 2),\n",
       " ('879', 2),\n",
       " ('881', 2),\n",
       " ('890', 2),\n",
       " ('897', 2),\n",
       " ('912', 2),\n",
       " ('916', 2),\n",
       " ('917', 2),\n",
       " ('930', 2),\n",
       " ('976', 2),\n",
       " ('977', 2),\n",
       " ('982', 2),\n",
       " ('1013', 2),\n",
       " ('1016', 2),\n",
       " ('1019', 2),\n",
       " ('1025', 2),\n",
       " ('1027', 2),\n",
       " ('1034', 2),\n",
       " ('1076', 2),\n",
       " ('1087', 2),\n",
       " ('1088', 2),\n",
       " ('1089', 2),\n",
       " ('1105', 2),\n",
       " ('1110', 2),\n",
       " ('472', 2),\n",
       " ('491', 2),\n",
       " ('498', 2),\n",
       " ('510', 2),\n",
       " ('515', 2),\n",
       " ('526', 2),\n",
       " ('540', 2),\n",
       " ('541', 2),\n",
       " ('546', 2),\n",
       " ('550', 2),\n",
       " ('554', 2),\n",
       " ('562', 2),\n",
       " ('566', 2),\n",
       " ('568', 2),\n",
       " ('572', 2),\n",
       " ('576', 2),\n",
       " ('578', 2),\n",
       " ('586', 2),\n",
       " ('590', 2),\n",
       " ('597', 2),\n",
       " ('599', 2),\n",
       " ('631', 2),\n",
       " ('636', 2),\n",
       " ('651', 2),\n",
       " ('1228', 2),\n",
       " ('1231', 2),\n",
       " ('1239', 2),\n",
       " ('1244', 2),\n",
       " ('1250', 2),\n",
       " ('1277', 2),\n",
       " ('1303', 2),\n",
       " ('1304', 2),\n",
       " ('1314', 2),\n",
       " ('1358', 2),\n",
       " ('1362', 2),\n",
       " ('1364', 2),\n",
       " ('1365', 2),\n",
       " ('1372', 2),\n",
       " ('1393', 2),\n",
       " ('1407', 2),\n",
       " ('1413', 2),\n",
       " ('1414', 2),\n",
       " ('1415', 2),\n",
       " ('1416', 2),\n",
       " ('1419', 2),\n",
       " ('1433', 2),\n",
       " ('1478', 2),\n",
       " ('1483', 2),\n",
       " ('1484', 2),\n",
       " ('1491', 2),\n",
       " ('1523', 2),\n",
       " ('1548', 2),\n",
       " ('1552', 2),\n",
       " ('1556', 2),\n",
       " ('1559', 2),\n",
       " ('1586', 2),\n",
       " ('1595', 2),\n",
       " ('1596', 2),\n",
       " ('1597', 2),\n",
       " ('1610', 2),\n",
       " ('1613', 2),\n",
       " ('1615', 2),\n",
       " ('1618', 2),\n",
       " ('1646', 2),\n",
       " ('1657', 2),\n",
       " ('1673', 2)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "movieid_genre = df_movies_cleaned.filter(lambda x: int(x[i]) == 1).map(lambda x: (str(x[0]), i))\n",
    "movieid_genre.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('546', (2, ('WY', 1.0))),\n",
       " ('515', (2, ('WY', 3.0))),\n",
       " ('298', (2, ('WY', 2.0))),\n",
       " ('127', (2, ('WY', 4.0)))]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieid_genre.join(movieid_state_avgrating).filter(lambda s: s[1][1][0]=='WY').collect()\n",
    "\n",
    "#genre_stateWithMaxAvgRating = movieid_genre_state_avgrating.reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New\n",
    "#Pseudocode:\n",
    "#for each genre:\n",
    "#  get list of movies in the above genre\n",
    "#  join list of movies with rating\n",
    "#  joining this with users to get zipcodes\n",
    "#  join this with zipcodes to get state\n",
    "#  reduce on basis of states to get average rating\n",
    "#  return the best average rating state(requirement of question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = sc.textFile('./moviedata/users.csv').map(lambda x: x.split(\",\"))\n",
    "df_zipcodes = sc.textFile('./moviedata/zipcodes.csv').map(lambda x: x.split(\",\"))\n",
    "df_rating = sc.textFile('./moviedata/rating.csv').map(lambda x: x.split(\",\"))\n",
    "df_movies = sc.textFile('./moviedata/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_movies(x):\n",
    "    y = x.split('\\\"')\n",
    "    return y[0].split(',')[0:1] + y[2].split(',')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieid_userid_rating = df_rating.map(lambda x: (x[1] , (x[0], float(x[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_zipcode = df_users.map(lambda x : (x[0], x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_state = df_zipcodes.map(lambda x: (x[0], x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'97', (u'253', 4.0)),\n",
       " (u'269', (u'284', 4.0)),\n",
       " (u'526', (u'106', 4.0)),\n",
       " (u'180', (u'121', 3.0)),\n",
       " (u'86', (u'62', 2.0))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieid_userid_rating.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean df_movies\n",
    "df_movies_cleaned = df_movies.map(lambda x: clean_df_movies(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre_id in range(1, 20):\n",
    "    #  get list of movies in the above genre\n",
    "    movie_list = df_movies_cleaned.filter(lambda x : int(x[genre_id]) == 1).map(lambda x : (x[0], genre_id))\n",
    "\n",
    "    #  join list of movies with rating\n",
    "    userid_rating_list = movie_list.join(movieid_userid_rating).map(lambda x : (x[1][1][0], x[1][1][1]))\n",
    "\n",
    "    #  joining this with users to get zipcodes\n",
    "    zipcode_rating_list = userid_rating_list.join(userid_zipcode).map(lambda x : (x[1][1], x[1][0]))\n",
    "\n",
    "    #  join this with zipcodes to get state\n",
    "    state_rating_list = zipcode_rating_list.join(zipcode_state).map(lambda x : (x[1][1], (x[1][0], 1)))\n",
    "\n",
    "    #  reduce on basis of states to get average rating\n",
    "    state_avg_rating_tuple = state_rating_list.reduceByKey(lambda x, y : (x[0]+y[0], x[1]+y[1]))\n",
    "    state_avg_rating = state_avg_rating_tuple.map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 191.0 failed 1 times, most recent failure: Lost task 0.0 in stage 191.0 (TID 450, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/rdd.py\", line 933, in func\n    acc = seqOp(acc, obj)\nTypeError: unbound method merge() must be called with NoneType instance as first argument (got StatCounter instance instead)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/rdd.py\", line 933, in func\n    acc = seqOp(acc, obj)\nTypeError: unbound method merge() must be called with NoneType instance as first argument (got StatCounter instance instead)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-1cc1a2685dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatcounter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstate_avg_rating\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, zeroValue, seqOp, combOp)\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 191.0 failed 1 times, most recent failure: Lost task 0.0 in stage 191.0 (TID 450, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/rdd.py\", line 933, in func\n    acc = seqOp(acc, obj)\nTypeError: unbound method merge() must be called with NoneType instance as first argument (got StatCounter instance instead)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python2.7/dist-packages/pyspark/rdd.py\", line 933, in func\n    acc = seqOp(acc, obj)\nTypeError: unbound method merge() must be called with NoneType instance as first argument (got StatCounter instance instead)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.statcounter import StatCounter\n",
    "state_avg_rating.aggregate(StatCounter(), StatCounter.merge, StatCounter.mergeStats).maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'217', 3.0),\n",
       " (u'207', 2.0),\n",
       " (u'60', 4.0),\n",
       " (u'268', 2.0),\n",
       " (u'102', 2.0),\n",
       " (u'87', 3.0),\n",
       " (u'279', 2.0),\n",
       " (u'222', 2.0),\n",
       " (u'94', 3.0),\n",
       " (u'267', 3.0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_user_id_rating_list.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
